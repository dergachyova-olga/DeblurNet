# ======================================================================================================================
# name        : config_sample.ini
# type        : sample default configuration file
# purpose     : defines parameters for main deblurnet.py script
# sections    : general - settings for the environment (randomness, logging, etc)
#             : data - describes input data and provides its location
#             : network - provides network's structural parameters common to all the network architectures
#             : train - parameters related to model's training
#             : results - parameters for saving of trained model
#             : unet, DnCNN, c2net, offresnet - parameters specific to these implemented network architectures
# author      : Olga Dergachyova
# last update : 10/2020
# ======================================================================================================================

[general]

# seed: seed for setting deterministic behaviour for data load
# log_to_file: prints all console messages into a file if True
# log_path: path for logger. Used if log_to_file is True

seed = None
log_to_file = False
log_path = log.txt


[data]

# train_data: a glob search pattern for finding all training input and output image matrices
# valid_data: a glob search pattern for finding all validation input and output image matrices
# visual_data: (optional) a glob search pattern for finding input and output image matrices to use for visual evaluation
# test_data: (optional) a glob search pattern for finding all test input and output image matrices
# input_suffix: (optional) suffix pattern for the input images
# output_suffix: (optional) suffix pattern for the output images (labels)
# multi_input: (optional) indicates if all *input.mat should be combined in one data sample
# multi_output: (optional) indicates if all *output.mat should be combined in one data sample
# order: (optional) indicates if multidimensional data should be stacked (RIRIRI...) or grouped by dimension (RRR...III...). Options: stacked, grouped
# n_vis: (optional) number of data samples to use for result visualization after each epoch
# valid: (optional) portion of train data to be used as validation set
# a_min: (optional) min value used for clipping. If None, -inf
# a_max: (optional) max value used for clipping. If None, inf

train_data = /home/user/data/train/*
valid_data = /home/user/data/valid/*
visual_data = /home/user/data/visual/*
test_data = /home/user/data/test/*
input_suffix = input.mat
output_suffix = output.mat
multi_input = False
multi_output = False
order = stacked
n_vis = 1
valid = 0.0
a_min = None
a_max = None


[network]

# name: network type. Options: unet, offresnet
# conv: convolution type. Options: 2D, 3D
# AF: (optional) activation function on intermediate conv layers. Options: relu, leaky_relu
# output_AF: (optional) activation function on last conv layers. Options: relu, leaky_relu, linear, tanh, sigmoid, norm
# kernel_size: (optional) size of the convolution kernel
# kernel_init: (optional) initializer for weights. Options: same, original, glorot, he, ortho
# keep_prob: (optional) dropout probability tensor indicating the part of neurons to keep if use_dropout = True
# use_dropout: (optional) flag to use dropout layers
# use_BN: (optional) flag to use batch normalization
# seed: (optional) seed for kernel initialization and dropout

name = unet
conv = 2D
AF = relu
output_AF = linear
kernel_size = 3
kernel_init = ortho
keep_prob = 0.75
use_dropout = True
use_BN = False
seed = None


[unet]

# layers: (optional) number of layers in unet
# features_root: (optional) number of features in the first layer
# pool_size: (optional) size of the max pooling operation
# upscale_method: (optional) method used in up side of the net to increase input size. Options: transpose, nn, bilinear, bicubic. If not transpose, interpolation + conv are used.
# use_skip: (optional) flag to use feature concatenation skip connections

layers = 5
features_root = 16
pool_size = 2
upscale_method = bicubic
use_skip = True

[DnCNN]

# middle_layers: (optional) number of layers between first and last Conv layers
# features: (optional) number of features in the every layer
# residual: (optional) defines if residual formulation is used
# BN_kwargs: (optional) keyword arguments for batch normalization

middle_layers = 5
features = 64
residual = True
BN_kwargs = {'axis': 3, 'momentum': 0.0, 'epsilon': 0.0001}


[c2net]

# layers: (optional) number of layers in network
# features_root: (optional) number of features in the first layer
# pool_size: (optional) size of the max pooling operation
# upscale_method: (optional) method used in up side of the net to increase input size. Options: transpose, nn, bilinear, bicubic. If not transpose, interpolation + conv are used.

layers = 3
features_root = 16
pool_size = 2
upscale_method = bicubic


[offresnet]

# blocks: (optional) number of residual blocks in network
# layers: (optional) number of convolutional layers in one residual block
# features: (optional) number of features in the each conv layer
# use_skip: (optional) flag to use feature concatenation skip connections

blocks = 1
layers = 2
features = 128
use_skip = True


[train]

# cost: name of cost function to minimize. Options: MSE, MAE, SSIM, MSE_intensity_weighted, MSE_Hermitian, MSE_Energy
# cost_reg: (optional) lambda for L2 regularization on trainable variables. If None, not used.
# optimizer: (optional) name of the optimizer to use. Options: momentum or adam
# optimizer_kwargs: (optional) additional parameters for optimizer. May contain learning_rate, decay_rate, and momentum
# train_batch_size: (optional) size of training batch
# valid_batch_size: (optional) size of validation batch
# training_iters: (optional) number of training batches per epoch. If None, all batches are used
# epochs: number of training epochs
# display_step: (optional) step to display training batch statistics
# patience: (optional) number of epochs to wait before stopping training if validation cost increases. If None, no early stopping
# restore: (optional) flag to restore pre-trained model

cost = MSE
cost_reg = None
optimizer = adam
optimizer_kwargs = {'learning_rate': 0.001}
train_batch_size = 128
valid_batch_size = 128
training_iters = None
epochs = 100
display_step = 20
patience = None
restore = False

[results]

# model_path: (optional) path to save or restore trained model
# prediction_path: (optional) path to save predictions made during train and test time
# summaries: (optional) flag to write tensor board summaries

model_path = ./convnet_trained
prediction_path = ./prediction
summaries = False
