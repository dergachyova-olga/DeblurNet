# ======================================================================================================================
# name        : config.ini
# type        : blank default configuration file
# purpose     : defines parameters for main deblurnet.py script (to be populated)
# sections    : general - settings for the environment (randomness, logging, etc)
#             : data - describes input data and provides its location
#             : network - provides network's structural parameters common to all the network architectures
#             : train - parameters related to model's training
#             : results - parameters for saving of trained model
#             : unet, DnCNN, c2net, offresnet - parameters specific to these implemented network architectures
# author      : Olga Dergachyova
# last update : 10/2020
# ======================================================================================================================

[general]

# seed: seed for setting deterministic behaviour for data load
# log_to_file: prints all console messages into a file if True
# log_path: path for logger. Used if log_to_file is True

seed =
log_to_file =
log_path =


[data]

# train_data: a glob search pattern for finding all training input and output image matrices
# valid_data: a glob search pattern for finding all validation input and output image matrices
# visual_data: (optional) a glob search pattern for finding input and output image matrices to use for visual evaluation
# test_data: (optional) a glob search pattern for finding all test input and output image matrices
# input_suffix: (optional) suffix pattern for the input images
# output_suffix: (optional) suffix pattern for the output images (labels)
# multi_input: (optional) indicates if all *input.mat should be combined in one data sample
# multi_output: (optional) indicates if all *output.mat should be combined in one data sample
# order: (optional) indicates if multidimensional data should be stacked (RIRIRI...) or grouped by dimension (RRR...III...). Options: stacked, grouped
# n_vis: (optional) number of data samples to use for result visualization after each epoch
# valid: (optional) portion of train data to be used as validation set
# a_min: (optional) min value used for clipping. If None, -inf
# a_max: (optional) max value used for clipping. If None, inf

train_data =
valid_data =
visual_data =
test_data =
input_suffix =
output_suffix =
multi_input =
multi_output =
order =
n_vis =
valid =
a_min =
a_max =


[network]

# name: network type. Options: unet, offresnet
# conv: convolution type. Options: 2D, 3D
# AF: (optional) activation function on intermediate conv layers. Options: relu, leaky_relu
# output_AF: (optional) activation function on last conv layers. Options: relu, leaky_relu, linear, tanh, sigmoid, norm
# kernel_size: (optional) size of the convolution kernel
# kernel_init: (optional) initializer for weights. Options: same, original, glorot, he, ortho
# keep_prob: (optional) dropout probability tensor indicating the part of neurons to keep if use_dropout = True
# use_dropout: (optional) flag to use dropout layers
# use_BN: (optional) flag to use batch normalization
# seed: (optional) seed for kernel initialization and dropout

name =
conv =
AF =
output_AF =
kernel_size =
kernel_init =
keep_prob =
use_dropout =
use_BN =
seed =


[unet]

# layers: (optional) number of layers in unet
# features_root: (optional) number of features in the first layer
# pool_size: (optional) size of the max pooling operation
# upscale_method: (optional) method used in up side of the net to increase input size. Options: transpose, nn, bilinear, bicubic. If not transpose, interpolation + conv are used.
# use_skip: (optional) flag to use feature concatenation skip connections

layers =
features_root =
pool_size =
upscale_method =
use_skip =

[DnCNN]

# middle_layers: (optional) number of layers between first and last Conv layers
# features: (optional) number of features in the every layer
# residual: (optional) defines if residual formulation is used
# BN_kwargs: (optional) keyword arguments for batch normalization

middle_layers =
features =
residual =
BN_kwargs =


[c2net]

# layers: (optional) number of layers in network
# features_root: (optional) number of features in the first layer
# pool_size: (optional) size of the max pooling operation
# upscale_method: (optional) method used in up side of the net to increase input size. Options: transpose, nn, bilinear, bicubic. If not transpose, interpolation + conv are used.

layers =
features_root =
pool_size =
upscale_method =


[offresnet]

# blocks: (optional) number of residual blocks in network
# layers: (optional) number of convolutional layers in one residual block
# features: (optional) number of features in the each conv layer
# use_skip: (optional) flag to use feature concatenation skip connections

blocks =
layers =
features =
use_skip =


[train]

# cost: name of cost function to minimize. Options: MSE, MAE, SSIM, MSE_intensity_weighted, MSE_Hermitian, MSE_Energy
# cost_reg: (optional) lambda for L2 regularization on trainable variables. If None, not used.
# optimizer: (optional) name of the optimizer to use. Options: momentum or adam
# optimizer_kwargs: (optional) additional parameters for optimizer. May contain learning_rate, decay_rate, and momentum
# train_batch_size: (optional) size of training batch
# valid_batch_size: (optional) size of validation batch
# training_iters: (optional) number of training batches per epoch. If None, all batches are used
# epochs: number of training epochs
# display_step: (optional) step to display training batch statistics
# patience: (optional) number of epochs to wait before stopping training if validation cost increases. If None, no early stopping
# restore: (optional) flag to restore pre-trained model

cost =
cost_reg =
optimizer =
optimizer_kwargs =
train_batch_size =
valid_batch_size =
training_iters =
epochs =
display_step =
patience =
restore =

[results]

# model_path: (optional) path to save or restore trained model
# prediction_path: (optional) path to save predictions made during train and test time
# summaries: (optional) flag to write tensor board summaries

model_path =
prediction_path =
summaries =
